{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification prediction using Machine Learning\n",
        "\n",
        "Python code that uses different Machine Learning algorithms to predict Malignant or Benign classification in the Wisconsin Breast Cancer dataset."
      ],
      "metadata": {
        "id": "bUCyxNf2YK_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load breast cancer Wisconsin dataset"
      ],
      "metadata": {
        "id": "82Nl4l39inuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "#load the sample data\n",
        "df=pd.read_csv('gdrive/My Drive/breast_cancer_wisconsin.csv')\n",
        "df.count()"
      ],
      "metadata": {
        "id": "WH3ktumfFopP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean the data to remove any null values"
      ],
      "metadata": {
        "id": "WCHyfQs2JRwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the last column \"Unnamed:32\", it does not have any values\n",
        "\n",
        "#axis = 1 means - Whether to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’).\n",
        "df = df.drop('Unnamed: 32', axis=1)\n",
        "\n",
        "#also drop the id column, as it is not relevant to prediction\n",
        "df = df.drop('id', axis=1)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "rMkRfPDEGL6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now chack the datatypes of all columns.\n",
        "\n",
        "We see that all columns are of type float64 except diagnosis. Diagnosis is our result variable, based on all the other float64 parameters. We will convert the object value of diagnosis into 1 and 0.\n"
      ],
      "metadata": {
        "id": "WnXCnq_nGv4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "SbylgcGEG0bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping M to 1 and B to 0 in the output Label DataFrame\n",
        "df['diagnosis']=df['diagnosis'].map({'M':1,'B':0})"
      ],
      "metadata": {
        "id": "UASz9RkfSok4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate result variable from input data"
      ],
      "metadata": {
        "id": "EFZoMLBacF8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df['diagnosis']\n",
        "\n",
        "#also drop the diagnosis column from the parameters, as it is the prediction\n",
        "df = df.drop('diagnosis', axis=1)"
      ],
      "metadata": {
        "id": "fe-XvvcvbgmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see all columns are of type float64 except diagnosis. Diagnosis i our result variable, based on all the other float64 parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "usNKmKwfG6pv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Correlation"
      ],
      "metadata": {
        "id": "mr2SbndTI1PS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation is a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). https://www.jmp.com/en_us/statistics-knowledge-portal/what-is-correlation.html\n",
        "\n",
        "If two variables are very strongly correlated (values close to either 1 or -1), they do not convey any extra information and should be removed from the dataset.\n",
        "\n",
        "We can find the correlation between different variables by creating a correlation matrix with all variables."
      ],
      "metadata": {
        "id": "0ZUuZ_YkcaQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.heatmap(df.corr(numeric_only = True).round(2),  annot=True)"
      ],
      "metadata": {
        "id": "2ntPLb-NI0kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding the data and correlation matrix"
      ],
      "metadata": {
        "id": "3-AjG-GjcV4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above, we see a few things -\n",
        "\n",
        "Firstly, the following properties are present in the data in three different views - mean, standard error and worst values-\n",
        "1.   redius\n",
        "2.   texture\n",
        "3.   perimeter\n",
        "4.   area\n",
        "5.   smoothness\n",
        "6.   compactness\n",
        "7.   concavity\n",
        "8.   concave_points\n",
        "9.   symmetry\n",
        "10.  fractal_dimension\n",
        "We will remove the columns that store worst and standard error.\n",
        "\n",
        "\n",
        "Secondly, the radius_mean is also closely correlated to -\n",
        "*   perimeter_mean\n",
        "*   area_mean\n",
        "So we will remove perimeter and area from our columns"
      ],
      "metadata": {
        "id": "hBRLWkGWLb10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Eliminating the highly correlated columns from out dataset\n",
        "\n",
        "# drop the _worst and _se columns\n",
        "columns = ['radius_worst',\n",
        "           'texture_worst',\n",
        "           'perimeter_worst',\n",
        "           'area_worst',\n",
        "           'smoothness_worst',\n",
        "           'compactness_worst',\n",
        "           'concavity_worst',\n",
        "           'concave points_worst',\n",
        "           'symmetry_worst',\n",
        "           'fractal_dimension_worst',\n",
        "           'radius_se',\n",
        "           'texture_se',\n",
        "           'perimeter_se',\n",
        "           'area_se',\n",
        "           'smoothness_se',\n",
        "           'compactness_se',\n",
        "           'concavity_se',\n",
        "           'concave points_se',\n",
        "           'symmetry_se',\n",
        "           'fractal_dimension_se',\n",
        "           ]\n",
        "df = df.drop(columns, axis=1)\n",
        "\n",
        "#drop the perimeter and area columns\n",
        "columns = ['perimeter_mean',\n",
        "           'area_mean',\n",
        "          ]\n",
        "df = df.drop(columns, axis=1)\n",
        "\n",
        "# tried dropping concavity and concave points columns, but that gives worse results, so keeping those columns\n",
        "#columns = ['concavity_se',\n",
        "#           'concave points_se',\n",
        "#           'fractal_dimension_se',\n",
        "#          ]\n",
        "#df = df.drop(columns, axis=1)\n",
        "\n",
        "# verify remaining columns\n",
        "df.columns\n"
      ],
      "metadata": {
        "id": "OljDYR62MTwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the correlation matrix again\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.heatmap(df.corr(numeric_only = True).round(2),  annot=True)"
      ],
      "metadata": {
        "id": "lAtO1IA4O6xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split the data into train and test\n",
        "Our data is now ready for training. We will split the data set into two pieces — a training set and a testing set.\n",
        "This consists of random sampling about 75 percent of the rows (can be varied) and putting them into a training set. The remaining 25 percent is put into a test set."
      ],
      "metadata": {
        "id": "qIpzJjm0Peb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets, first storing diagnosis for the Y-axis\n",
        "X = df\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=40)\n",
        "\n",
        "#Let us check the dimensions of or split dataset\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "x_train.head()"
      ],
      "metadata": {
        "id": "SEECJOLJQS_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a method to print results in desired format"
      ],
      "metadata": {
        "id": "DD0gj_Bsc6ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# print false negatives\n",
        "def printResults(y_test, y_predict):\n",
        "  # Calculate confusion matrix\n",
        "  conf_matrix = confusion_matrix(y_test, y_predict)\n",
        "  print('Classification Report : \\n', classification_report(y_test, y_predict, digits=3), '\\n')\n",
        "  print('----------------------------------------------------------------\\n')\n",
        "  print('confusion matrix : \\n', conf_matrix, '\\n')\n",
        "  print('----------------------------------------------------------------\\n')\n",
        "  print('True Negative:', conf_matrix[0][0])\n",
        "  print('False Positive:', conf_matrix[0][1])\n",
        "  print('False Negative:', conf_matrix[1][0])\n",
        "  print('True Positive:', conf_matrix[1][1])"
      ],
      "metadata": {
        "id": "1OalJ7AmvuQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scale the data"
      ],
      "metadata": {
        "id": "6tIga2ZIdB7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling the data(feature scaling)\n",
        "# This can help to balance the impact of all variables on the distance\n",
        "# calculation and can help to improve the performance of the algorithm.\n",
        "# In particular, several ML techniques, such as neural networks, require\n",
        "# that the input data to be normalized for it to work well.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.fit_transform(x_test)"
      ],
      "metadata": {
        "id": "L537xjdYXwZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Logistic Regression"
      ],
      "metadata": {
        "id": "4knVhr7RkE7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "lr=LogisticRegression(random_state=16)\n",
        "lr.fit(x_train, y_train)\n",
        "\n",
        "y_predict = lr.predict(x_test)\n",
        "printResults(y_test, y_predict)\n",
        "print('\\n-----------------------------------------------------\\n')\n",
        "print('Accuracy of classifier on test set: {:.2f}'.format(lr.score(x_test, y_test)))"
      ],
      "metadata": {
        "id": "MAqR2hlqjk2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Logistic Regression with stochastic gradient descent"
      ],
      "metadata": {
        "id": "L-W0-4VlkIoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#logistic regression with stochastic gradient decent\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd=SGDClassifier()\n",
        "sgd.fit(x_train, y_train)\n",
        "\n",
        "#### Now print results\n",
        "y_predict = sgd.predict(x_test)\n",
        "printResults(y_test, y_predict)\n",
        "print('\\n-----------------------------------------------------\\n')\n",
        "print('Accuracy of classifier on test set: {:.2f}'.format(sgd.score(x_test, y_test)))"
      ],
      "metadata": {
        "id": "qRMgnbn9kS_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Support Vector Machine"
      ],
      "metadata": {
        "id": "XbZS6RfXkhSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm=LinearSVC(C=0.01)\n",
        "svm.fit(x_train, y_train)\n",
        "\n",
        "#### Now print results\n",
        "y_predict = svm.predict(x_test)\n",
        "printResults(y_test, y_predict)\n",
        "print('\\n-----------------------------------------------------\\n')\n",
        "print('Accuracy of classifier on test set: {:.2f}'.format(svm.score(x_test, y_test)))"
      ],
      "metadata": {
        "id": "aaV7_-vikm9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Random Forest"
      ],
      "metadata": {
        "id": "2fRNxgFtkwKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# n_estimators = number of desission trees\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(x_train, y_train)\n",
        "\n",
        "#### Now print results\n",
        "y_predict = rf.predict(x_test)\n",
        "printResults(y_test, y_predict)\n",
        "\n",
        "print('\\n-----------------------------------------------------\\n')\n",
        "print('Accuracy of classifier on test set: {:.2f}'.format(rf.score(x_test, y_test)))"
      ],
      "metadata": {
        "id": "07_kq7CEkwdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Decision Tree"
      ],
      "metadata": {
        "id": "O7J3DqN5lEcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "#### Now print results\n",
        "y_predict = clf.predict(x_test)\n",
        "printResults(y_test, y_predict)\n",
        "\n",
        "print('\\n-----------------------------------------------------\\n')\n",
        "print('Accuracy of classifier on test set: {:.2f}'.format(clf.score(x_test, y_test)))"
      ],
      "metadata": {
        "id": "H3mkgbdRlGri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Boosted Decision Tree"
      ],
      "metadata": {
        "id": "Z3ev332FlKDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "adb = AdaBoostClassifier()\n",
        "adb.fit(x_train, y_train)\n",
        "\n",
        "#### Now print results\n",
        "y_predict = adb.predict(x_test)\n",
        "printResults(y_test, y_predict)\n",
        "\n",
        "print('\\n-----------------------------------------------------\\n')\n",
        "print('Accuracy of classifier on test set: {:.2f}'.format(adb.score(x_test, y_test)))"
      ],
      "metadata": {
        "id": "Lxs8cft8lNJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Neural Network"
      ],
      "metadata": {
        "id": "oarvlD-iUk36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LeakyReLU,PReLU,ELU\n",
        "from keras.layers import Dropout\n",
        "\n",
        "#creating model\n",
        "classifier = Sequential()\n",
        "\n",
        "# Now we create the layers of the neural network:\n",
        "# Input layer\n",
        "# Hidden layer\n",
        "# Output layer\n",
        "\n",
        "#first hidden layer\n",
        "classifier.add(Dense(units=9,kernel_initializer='he_uniform',activation='relu',input_dim=8))\n",
        "#second hidden layer\n",
        "classifier.add(Dense(units=9,kernel_initializer='he_uniform',activation='relu'))\n",
        "# last layer or output layer\n",
        "classifier.add(Dense(units=1,kernel_initializer='glorot_uniform',activation='sigmoid'))\n",
        "\n",
        "#taking summary of layers\n",
        "classifier.summary()\n",
        "\n",
        "#compiling the ANN\n",
        "classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        " #fitting the ANN to the training set\n",
        "model = classifier.fit(x_train,y_train,batch_size=100,epochs=100)\n",
        "\n",
        "#now testing for Test data\n",
        "y_predict = classifier.predict(x_test)\n",
        "\n",
        "y_predict = np.where(y_predict > 0.5, 1, 0)\n",
        "\n",
        "#print(y_predict)\n",
        "printResults(y_test, y_predict)"
      ],
      "metadata": {
        "id": "G9qbZLDA0w03"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}